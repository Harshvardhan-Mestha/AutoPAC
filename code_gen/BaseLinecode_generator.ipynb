{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVQHDRRUev_7",
        "outputId": "0746ce59-6f5e-425c-a04d-ae03843fc867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: openai in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (1.34.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: certifi in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: google.generativeai in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (0.6.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.4 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google.generativeai) (0.6.4)\n",
            "Requirement already satisfied: google-api-core in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google.generativeai) (2.19.0)\n",
            "Requirement already satisfied: google-api-python-client in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google.generativeai) (2.133.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google.generativeai) (2.30.0)\n",
            "Requirement already satisfied: protobuf in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google.generativeai) (4.25.3)\n",
            "Requirement already satisfied: pydantic in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google.generativeai) (2.7.4)\n",
            "Requirement already satisfied: tqdm in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google.generativeai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google.generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google-ai-generativelanguage==0.6.4->google.generativeai) (1.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core->google.generativeai) (1.63.1)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core->google.generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google-auth>=2.15.0->google.generativeai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google-auth>=2.15.0->google.generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google-auth>=2.15.0->google.generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google-api-python-client->google.generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google-api-python-client->google.generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google-api-python-client->google.generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from pydantic->google.generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from pydantic->google.generativeai) (2.18.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->google.generativeai) (0.4.6)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google.generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google.generativeai) (1.62.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google.generativeai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google.generativeai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\win 11\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (2024.6.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install openai\n",
        "%pip install google.generativeai\n",
        "import openai\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import subprocess\n",
        "import os\n",
        "import pathlib\n",
        "import textwrap\n",
        "import subprocess\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import pathlib\n",
        "import textwrap\n",
        "# from  google.colab import drive\n",
        "# from google.colab import userdata\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r0dnS9p83PEl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('df_train_shuffled.csv' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evuZ012ee0fR",
        "outputId": "1638f18b-dc01-4db7-df4e-2afd4e3c704b"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f-5HD28Ue0rQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:1: SyntaxWarning: invalid escape sequence '\\L'\n",
            "<>:27: SyntaxWarning: invalid escape sequence '\\L'\n",
            "<>:1: SyntaxWarning: invalid escape sequence '\\L'\n",
            "<>:27: SyntaxWarning: invalid escape sequence '\\L'\n",
            "C:\\Users\\WIN 11\\AppData\\Local\\Temp\\ipykernel_25180\\2832552133.py:1: SyntaxWarning: invalid escape sequence '\\L'\n",
            "  with open('D:\\LLM_GPT\\APPCAIR\\expDetails.json','r') as openfile:\n",
            "C:\\Users\\WIN 11\\AppData\\Local\\Temp\\ipykernel_25180\\2832552133.py:27: SyntaxWarning: invalid escape sequence '\\L'\n",
            "  with open('D:\\LLM_GPT\\APPCAIR\\expDetails.json','w') as openfile:\n"
          ]
        }
      ],
      "source": [
        "#path to your folder which contains the json and other files\n",
        "with open('D:\\LLM_GPT\\APPCAIR\\expDetails.json','r') as openfile:\n",
        "  expDetails = json.load(openfile)\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "GOOGLE_API_KEY=expDetails['GeminiAPIKEY']\n",
        "#CLAUDE_API_KEY= expDetails['ClaudeAPIKEY']\n",
        "GPT_API_KEY = expDetails['GPTAPIKEY']\n",
        "os.environ['OPENAI_API_KEY'] = GPT_API_KEY\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "Gmodel = genai.GenerativeModel(expDetails['GeminiModel'])\n",
        "\n",
        "GPTModel = expDetails['GPTModel']\n",
        "openai.api_key = GPT_API_KEY\n",
        "client = OpenAI()\n",
        "\n",
        "RunCount = expDetails['RunCount']\n",
        "RunCount = str(RunCount)\n",
        "EXP_NAME = expDetails['ExpName']\n",
        "json_object = json.dumps(expDetails,indent = 4)\n",
        "with open('D:\\LLM_GPT\\APPCAIR\\expDetails.json','w') as openfile:\n",
        "  openfile.write(json_object)\n",
        "\n",
        "def fileReader(pathToFile):\n",
        "  f = open(pathToFile,'r')\n",
        "  s = f.read()\n",
        "  f.close()\n",
        "  return s\n",
        "\n",
        "idea = fileReader(expDetails['ideaFilePath'])\n",
        "context = fileReader(expDetails['contextFilePath'])\n",
        "libraries = fileReader(expDetails['librariesPath'])\n",
        "methodology =fileReader(expDetails['methodology'])\n",
        "miscellaneous= '' #fileReader(expDetails['miscellenous'])\n",
        "idea = \"\\n\\n\"+ idea+\"\\n\\n\"+ methodology\n",
        "custom_dataset=expDetails['dataset']\n",
        "packages=fileReader(expDetails['packages'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-s56wxVe4uW",
        "outputId": "390f1d12-6cf2-4bf9-e991-e85fd5d783a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pip', 'install', '-q', 'numerapi', 'umap-learn', 'pandas', 'pyarrow', 'matplotlib', 'lightgbm', 'scikit-learn', 'cloudpickle', 'scipy==1.10.1', 'tabpfn'], returncode=1)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import subprocess\n",
        "# Read the contents of the text file\n",
        "packages = packages.replace('!', '')\n",
        "# Execute pip install commands\n",
        "subprocess.run(packages.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YuTGXFonhc_5"
      },
      "outputs": [],
      "source": [
        "SELF_CONSISTENCY = 5\n",
        "chats = [None]*SELF_CONSISTENCY\n",
        "responses = [None]*SELF_CONSISTENCY\n",
        "for i in range(SELF_CONSISTENCY):\n",
        "  chat  = Gmodel.start_chat(history=[])\n",
        "  chats[i] = chat\n",
        "InstructionModel = expDetails['InstructionGenerationModel']\n",
        "InstructionModel = InstructionModel.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "og_k2nvhj2Zb"
      },
      "outputs": [],
      "source": [
        "InstructionModel='gpt' #if you want to use gpt4 for pipeline generation\n",
        "modifiedPrelude = \"\"\"\n",
        "  You will be provided with an idea and the necessary libraries. Your task is to outline a detailed pipeline without giving actual code.\n",
        "\n",
        "  Review the provided context and idea carefully.\n",
        "  Do not include the code; focus on describing the pipeline.\n",
        "  Explain each step thoroughly, considering the dataset characteristics and the proposed approach.\n",
        "  Utilize the provided libraries as necessary.\n",
        "  Aim for clarity and coherence in your response.\n",
        "\n",
        "  Approach this as a Machine Learning Researcher, providing a step-by-step plan for the analysis.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "if InstructionModel == 'gemini':\n",
        "    responses = [None] * SELF_CONSISTENCY\n",
        "    for i in range(SELF_CONSISTENCY):\n",
        "        try:\n",
        "            responses[i] = Gmodel.generate_content(f\"\"\"\n",
        "            {modifiedPrelude}\n",
        "            [Idea]:{idea}\n",
        "            \"\"\", safety_settings={'HARASSMENT': 'block_none'})\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred with 'gemini' model: {e}\")\n",
        "            InstructionModel = 'gpt'\n",
        "            break\n",
        "\n",
        "if InstructionModel == 'gpt':\n",
        "    responses = [None] * SELF_CONSISTENCY\n",
        "    for i in range(SELF_CONSISTENCY):\n",
        "        responses[i] = client.chat.completions.create(\n",
        "            model=GPTModel,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a Machine Learning Researcher. You will be provided with an idea and the necessary libraries. Your task is to outline a detailed pipeline without giving actual code.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"\"\"\n",
        "                {modifiedPrelude}\n",
        "                [Idea]:{idea}\n",
        "                \"\"\"}\n",
        "            ]\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUBD8FNdyYt7",
        "outputId": "3a547135-b196-4f5d-f1f2-00dc40d931e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-9gG04lLRKwjPVAz3y4WXMFEdGc4SC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The proposed methodology and its refinement offer a comprehensive plan for analyzing the performance of neural networks and gradient boosted decision trees on a tabular stock market dataset. The methodology correctly identifies key steps such as preprocessing, feature engineering, metafeature calculation, model training and evaluation, as well as metafeature analysis. This structured approach ensures a thorough comparison between different algorithms and provides insights into how dataset characteristics influence algorithm performance. Below are further elaborations and considerations for each step to ensure clarity and effectiveness in execution:\\n\\n1. **Preprocess the Dataset**:\\n    - **Missing Values**: Implement strategies based on the nature of missing data. For example, median imputation for skewed distributions or mean imputation for normally distributed features. Consider using models that can inherently handle missing values, like XGBoost.\\n    - **Categorical Features**: Utilize one-hot encoding for features with a low cardinality; for high cardinality features, consider target encoding.\\n    - **Temporal Splits**: Ensure data splits respect the temporal order, avoiding future information in the training set, which prevents data leakage and better simulates real-world forecasting.\\n\\n2. **Feature Engineering**:\\n    - Focus on extracting time-series specific features that could reveal trends, seasonality, and cyclical patterns, which are crucial for stock market data. Features such as moving averages, exponential moving averages, and differences between lags can be particularly informative.\\n    - Consider the stationarity of your time-series data and apply transformations as necessary, such as differencing or logarithmic transformations, to stabilize variance and reduce trend components.\\n\\n3. **Calculate Metafeatures**:\\n    - When calculating metafeatures, consider not only global dataset characteristics but also those that are specific to financial time series, such as volatility measures, average trade volume changes, and price/earnings ratios.\\n    - This step can be enriched by incorporating domain-specific knowledge to select or construct metafeatures that are particularly indicative of the dataset's predictive complexity for financial data.\\n\\n4. **Train and Evaluate Models**:\\n    - For neural networks, consider architectures that are specifically designed for sequence data, such as LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Units), alongside TabNet or TabTransformer. This can capture temporal dependencies more effectively than standard MLPs.\\n    - In hyperparameter tuning, leverage techniques like Bayesian Optimization for efficiency, especially given the potentially large search space and computational expense.\\n    - Employ cross-validation techniques suitable for time series, such as forward chaining, to ensure robust evaluation.\\n\\n5. **Compare Performance**:\\n    - Use a variety of metrics that capture different aspects of model performance. Consider including domain-specific evaluation metrics like the Sharpe ratio, which measures risk-adjusted return, and drawdown, which measures the decline from a peak in net asset value, in addition to standard metrics like accuracy, log-loss, and F1-score.\\n\\n6. **Analyze Metafeature Correlations**:\\n    - Deep dive into the relationships between the metafeatures and model performance. This analysis will likely reveal insights into why certain models perform better on specific types for datasets and guide the selection of models for similar future datasets.\\n    - The insights derived can inform not only the choice of algorithms but also preprocessing steps and feature engineering for similar datasets.\\n\\n7. **Reporting**:\\n    - Highlight both quantitative and qualitative findings. Include visualizations of performance metrics, feature importance scores, and metafeature impacts on algorithm performance.\\n    - Discuss in detail the insights gained regarding the interaction between dataset characteristics and model efficacy. Include a section on potential biases and limitations in the study to lend credibility and outline avenues for future research.\\n\\nBy following this detailed plan, researchers can systemically assess and understand the comparative strengths and weaknesses of using neural networks versus gradient boosted decision trees for stock market classification tasks. Additionally, this methodology fosters a deeper understanding of how specific characteristics of financial time series data influence predictive modeling success, contributing valuable insights to the field of machine learning in finance.\", role='assistant', function_call=None, tool_calls=None))], created=1719858308, model='gpt-4-0125-preview', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=796, prompt_tokens=1756, total_tokens=2552))\n",
            "here4\n",
            "Script executed successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "InstructionModel='gpt'\n",
        "# Function to execute the script and return True if successful, False otherwise\n",
        "def run_script():\n",
        "  if(InstructionModel=='gemini'):\n",
        "    try:\n",
        "        # Define the self consistency prompt\n",
        "        selfConsistencyPrompt = f\"\"\"I have generated the following responses to the following responses to the question:{modifiedPrelude}\n",
        "          [Context]:{context}\n",
        "          [Idea]:{idea}\n",
        "           \"\"\"\n",
        "        # Append generated responses to the prompt\n",
        "        for i in range(SELF_CONSISTENCY):\n",
        "            selfConsistencyPrompt += f\"\"\"Response {i}:\n",
        "\n",
        "          {responses[i].text}\n",
        "          \"\"\"\n",
        "\n",
        "        # Append final prompt\n",
        "        selfConsistencyPrompt += \"\"\"\n",
        "    Pick the most consistent response based on the majority consensus and give reason for choosing why that response\n",
        "    \"\"\"\n",
        "\n",
        "        # Generate final response\n",
        "        finalResponse = Gmodel.generate_content(selfConsistencyPrompt, safety_settings={'HARASSMENT':'block_none'})\n",
        "\n",
        "        # Extract response number from the text\n",
        "        Res = Gmodel.generate_content(f\"Extract the response number from the text. Be very concise and output only and only the response number and nothing else {finalResponse.text}\", safety_settings={'HARASSMENT':'block_none'})\n",
        "\n",
        "        # Get instruction based on selected response\n",
        "        Instruction = responses[int(Res.text)].text\n",
        "\n",
        "        Instruction=\"\"\n",
        "        if(InstructionModel==\"gpt\"):\n",
        "          # Get instruction based on selected response\n",
        "          Instruction = responses[int(Res.choices[0].message.content)]\n",
        "        else:\n",
        "          Instruction = responses[int(Res.text)]\n",
        "        print(Instruction)  \n",
        "        Pipeline=Instruction.choices[0].message.content     \n",
        "        base_folder = expDetails['FolderName'].lstrip('/').replace('/', '\\\\')  # Convert forward slashes to backslashes\n",
        "        expname = expDetails['ExpName'] + RunCount\n",
        "        # Write instruction to pipeline.md in the created folder\n",
        "        filepath = os.path.join(base_folder, expname, 'pipeline.md')\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "        with open(filepath,'w') as f:\n",
        "            print(\"here4\")\n",
        "            f.write(Pipeline)\n",
        "\n",
        "        return True  \n",
        "    except Exception as e:\n",
        "        print(\"Error occurred:\", e)\n",
        "        InstructionModel=='gpt'\n",
        "          # Return False if there's an error\n",
        "\n",
        "  if(InstructionModel=='gpt'):\n",
        "      try:\n",
        "        # Define the self consistency prompt\n",
        "        selfConsistencyPrompt = f\"\"\"I have generated the following responses to the following responses to the question:{modifiedPrelude}\n",
        "          [Context]:{context}\n",
        "          [Idea]:{idea}\n",
        "           \"\"\"\n",
        "\n",
        "        # Append generated responses to the prompt\n",
        "        for i in range(SELF_CONSISTENCY):\n",
        "            selfConsistencyPrompt += f\"\"\"Response {i}:\n",
        "\n",
        "          {responses[i]}\n",
        "          \"\"\"\n",
        "\n",
        "        # Append final prompt\n",
        "        selfConsistencyPrompt += \"\"\"\n",
        "    Pick the most consistent response based on the majority consensus and give reason for choosing why that response\n",
        "    \"\"\"\n",
        "\n",
        "        # Generate final response\n",
        "        #finalResponse = Gmodel.generate_content(selfConsistencyPrompt, safety_settings={'HARASSMENT':'block_none'})\n",
        "        finalResponse = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\":\"system\",\n",
        "                \"content\":f\"You are a Machine Learning engineer. Your task is to Pick the most consistent response based on the majority consensus and give reason for choosing why that response.\"\n",
        "            },\n",
        "               {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"selfConsistencyPrompt:{selfConsistencyPrompt}\\n\\n.Pick the most consistent response based on the majority consensus and give reason for choosing why that response.\"\n",
        "               }\n",
        "\n",
        "\n",
        "        ]\n",
        "\n",
        "    )\n",
        "        # Extract response number from the text\n",
        "        #Res = Gmodel.generate_content(f\"Extract the response number from the text. Be very concise and output only and only the response number and nothing else {finalResponse.text}\", safety_settings={'HARASSMENT':'block_none'})\n",
        "        Res = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo\",\n",
        "        messages=[\n",
        "               {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"finalResponse:{finalResponse.choices[0].message.content}\\n\\n.Extract the response number from the finalResponse text. Be very concise and output only and only the response number and nothing else.\"\n",
        "               }\n",
        "\n",
        "\n",
        "        ]\n",
        "\n",
        "    )\n",
        "        \n",
        "        Instruction=\"\"\n",
        "        if(InstructionModel==\"gpt\"):\n",
        "          # Get instruction based on selected response\n",
        "          Instruction = responses[int(Res.choices[0].message.content)]\n",
        "        else:\n",
        "          Instruction = responses[int(Res.text)]\n",
        "        print(Instruction)  \n",
        "        Pipeline=Instruction.choices[0].message.content     \n",
        "        base_folder = expDetails['FolderName'].lstrip('/').replace('/', '\\\\')  # Convert forward slashes to backslashes\n",
        "        expname = expDetails['ExpName'] + RunCount\n",
        "        # Write instruction to pipeline.md in the created folder\n",
        "        filepath = os.path.join(base_folder, expname, 'pipeline.md')\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "        with open(filepath,'w') as f:\n",
        "            print(\"here4\")\n",
        "            f.write(Pipeline)\n",
        "        return True  \n",
        "      except Exception as e:\n",
        "        print(\"Error occurred:\", e)\n",
        "while True:\n",
        "    if run_script():\n",
        "        print(\"Script executed successfully.\")\n",
        "        break\n",
        "    else:\n",
        "        print(\"Error encountered. Retrying...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bddt1xPVltVm"
      },
      "outputs": [],
      "source": [
        "base_folder = expDetails['FolderName'].lstrip('/').replace('/', '\\\\')  # Convert forward slashes to backslashes\n",
        "expname = expDetails['ExpName'] + RunCount\n",
        "filepath = os.path.join(base_folder, expname, 'pipeline.md')\n",
        "Pipeline=\"\"\n",
        "with open(filepath, 'r') as f:\n",
        "  Pipeline=f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXYEizLQPt-E"
      },
      "source": [
        "Code generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = pd.read_csv(custom_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjZ4dpJtj5am",
        "outputId": "e6356140-5e8d-47da-da91-86ecb1e879f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the comprehensive methodology outlined above, let's now implement the Python code for each step in detail. We'll work through data preprocessing, feature engineering, model training, evaluation, and analysis without omitting any steps or using placeholders.\n",
            "\n",
            "### Step 1: Preprocess the Data\n",
            "\n",
            "```python\n",
            "import pandas as pd\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
            "from sklearn.impute import SimpleImputer\n",
            "\n",
            "def preprocess_data(data):\n",
            "    # Handle missing values\n",
            "    imputer = SimpleImputer(strategy='mean')  # or median based on distribution\n",
            "    data.iloc[:, :-1] = imputer.fit_transform(data.iloc[:, :-1])\n",
            "    \n",
            "    # Encode categorical features\n",
            "    # Assuming categorical variables are of type 'object'\n",
            "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
            "    encoder = OneHotEncoder(sparse=False)\n",
            "    categorical_data = encoder.fit_transform(data[categorical_cols])\n",
            "    categorical_df = pd.DataFrame(categorical_data, columns=encoder.get_feature_names(categorical_cols))\n",
            "    data = data.drop(categorical_cols, axis=1)\n",
            "    data = pd.concat([data, categorical_df], axis=1)\n",
            "    \n",
            "    # Split data into train/val/test sets\n",
            "    train_val, test = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)\n",
            "    train, val = train_test_split(train_val, test_size=0.25, random_state=42, shuffle=False) # 0.25 x 0.8 = 0.2\n",
            "    \n",
            "    # Scale features\n",
            "    scaler = StandardScaler()\n",
            "    train_scaled = scaler.fit_transform(train.iloc[:, :-1])\n",
            "    val_scaled = scaler.transform(val.iloc[:, :-1])\n",
            "    test_scaled = scaler.transform(test.iloc[:, :-1])\n",
            "    \n",
            "    return train_scaled, val_scaled, test_scaled, train.iloc[:, -1], val.iloc[:, -1], test.iloc[:, -1]\n",
            "\n",
            "# Example usage:\n",
            "# train_scaled, val_scaled, test_scaled, y_train, y_val, y_test = preprocess_data(train)\n",
            "```\n",
            "\n",
            "### Step 2: Feature Engineering\n",
            "\n",
            "```python\n",
            "import numpy as np\n",
            "\n",
            "def engineer_features(data):\n",
            "    # Example: Add rolling mean features\n",
            "    rolling_windows = [5, 10, 20]\n",
            "    for window in rolling_windows:\n",
            "        rolling_mean = data.rolling(window=window).mean()\n",
            "        data[f'rolling_mean_{window}'] = rolling_mean\n",
            "    return data\n",
            "\n",
            "# Example usage:\n",
            "# train_features = engineer_features(pd.DataFrame(train_scaled))\n",
            "# val_features = engineer_features(pd.DataFrame(val_scaled))\n",
            "# test_features = engineer_features(pd.DataFrame(test_scaled))\n",
            "```\n",
            "\n",
            "### Step 3: Calculate Metafeatures\n",
            "\n",
            "```python\n",
            "from pymfe.mfe import MFE\n",
            "\n",
            "def extract_metafeatures(data):\n",
            "    mfe = MFE(groups=[\"statistical\", \"info-theory\", \"model-based\"])\n",
            "    mfe.fit(data.values, y_train)\n",
            "    metafeatures = mfe.extract()\n",
            "    return metafeatures\n",
            "\n",
            "# Example usage:\n",
            "# metafeatures = extract_metafeatures(train_features)\n",
            "```\n",
            "\n",
            "### Step 4: Train and Evaluate Models\n",
            "\n",
            "```python\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.metrics import accuracy_score, log_loss\n",
            "\n",
            "def train_and_evaluate(train_data, val_data, y_train, y_val):\n",
            "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
            "    model.fit(train_data, y_train)\n",
            "    y_pred = model.predict(val_data)\n",
            "    y_prob = model.predict_proba(val_data)\n",
            "    acc = accuracy_score(y_val, y_pred)\n",
            "    loss = log_loss(y_val, y_prob)\n",
            "    return acc, loss\n",
            "\n",
            "# Example usage:\n",
            "# accuracy, log_loss = train_and_evaluate(train_features, val_features, y_train, y_val)\n",
            "```\n",
            "\n",
            "### Step 5: Metafeature Analysis\n",
            "\n",
            "```python\n",
            "def metafeature_analysis(metafeatures):\n",
            "    # Example: Print metafeatures\n",
            "    print(metafeatures)\n",
            "\n",
            "# Example usage:\n",
            "# metafeature_analysis(metafeatures)\n",
            "```\n",
            "\n",
            "### Step 6: Reporting\n",
            "\n",
            "```python\n",
            "def report_results(accuracy, loss):\n",
            "    print(f\"Accuracy: {accuracy}\")\n",
            "    print(f\"Log Loss: {loss}\")\n",
            "\n",
            "# Example usage:\n",
            "# report_results(accuracy, log_loss)\n",
            "```\n",
            "\n",
            "This code provides a full implementation of the methodology, from data preprocessing to model evaluation and reporting, adhering to the framework previously outlined. Each function is designed to be flexible and can be adapted or extended based on specific dataset characteristics or additional requirements.\n"
          ]
        }
      ],
      "source": [
        "completion = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\":\"system\",\n",
        "                \"content\":f\"You are a Machine Learning engineer. Your task is to generate very detailed & Complete Python code based on end to end.Don't ommit anything due to breveity , Don't miss out on any steps.Do all complicated implementations \"\n",
        "            },\n",
        "               {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"idea:{idea} \\n\\n pipeline:{Pipeline}\\n\\n\\n miscellenous:{miscellaneous} \\n\\n..Assume the data is already loaded in train variable.Follow the instructions clearly.Do all the complicated implementations.Follow the [Instruction];{Pipeline} strictly.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Do all implementations without omission ignoring practical constraints.Don't simulate anything or don't include any Placeholders.Don't do brevity or use words like #pass\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\" [Idea]:{idea}\\n\\n Dataset_loading: Assume the data is already loaded in 'train'. The training data is stored in *train* variable.\\n\\n \"\n",
        "            }\n",
        "\n",
        "\n",
        "        ],\n",
        "        temperature=0.5\n",
        "\n",
        "    )\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6bwdDZnj7WG",
        "outputId": "a512de5b-cc45-4580-e93f-0ec28492f044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Message content has been saved to 'completion[0].txt' file.\n",
            "Message content has been saved to 'completion[0].txt' file.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "with open('completion[0].txt', 'w') as f:\n",
        "    f.write(completion.choices[0].message.content)\n",
        "\n",
        "# Print a confirmation message\n",
        "print(\"Message content has been saved to 'completion[0].txt' file.\")\n",
        "\n",
        "\n",
        "# Print a confirmation message\n",
        "print(\"Message content has been saved to 'completion[0].txt' file.\")\n",
        "\n",
        "# Read the content of the text file\n",
        "with open('completion[0].txt', 'r') as file:\n",
        "    text_content = file.read()\n",
        "\n",
        "# Extract Python code blocks using regular expressions\n",
        "python_code_blocks = re.findall(r'```python\\s*([\\s\\S]+?)\\s*```', text_content)\n",
        "\n",
        "# Remove pip installs from Python code blocks\n",
        "python_code_blocks_cleaned = []\n",
        "for code_block in python_code_blocks:\n",
        "    cleaned_block = re.sub(r'^!pip install .*\\n', '', code_block, flags=re.MULTILINE)\n",
        "    python_code_blocks_cleaned.append(cleaned_block)\n",
        "\n",
        "# Write the cleaned Python code blocks to a new file\n",
        "with open('extracted_code_cleaned.py', 'w') as file:\n",
        "    for code_block in python_code_blocks_cleaned:\n",
        "        file.write(code_block + '\\n\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "Code=completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBTa4m2KlG6l",
        "outputId": "bd21693d-8f72-40e4-ad88-0700dfd8e31c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "[]\n",
            "         \n",
            "Code executed successfully. No errors.\n",
            "Successfully wrote to D:\\LLM_GPT\\APPCAIR\\TS_mixer17\\Basecode.py\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:68: SyntaxWarning: invalid escape sequence '\\L'\n",
            "<>:68: SyntaxWarning: invalid escape sequence '\\L'\n",
            "C:\\Users\\WIN 11\\AppData\\Local\\Temp\\ipykernel_25180\\335155212.py:68: SyntaxWarning: invalid escape sequence '\\L'\n",
            "  with open('D:\\LLM_GPT\\APPCAIR\\expDetails.json', 'w') as json_file:\n"
          ]
        }
      ],
      "source": [
        "import traceback\n",
        "import re\n",
        "def debug_error(error_message, code):\n",
        "    # Define the prompt with the error message\n",
        "    prompt = f\"Error message: {error_message}\\n\\nDebugging suggestions:\"\n",
        "\n",
        "    # Generate suggestions using GPT\n",
        "    completion = client.chat.completions.create(\n",
        "    model=\"gpt-4-turbo-preview\",\n",
        "    system=\"You are a data scientist.I have encountered an error.Please fix the error in a logical way and dont assume anything.Regenerate the entire code again.If you need any clarifications ask.Output the entire code again after error rectification\"\"\",\n",
        "    messages=[       {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a data scientist.I have encountered an error.Please fix the error in a logical way and don't assume anything.Regenerate the entire code again.Output the entire rectified code again after error rectification\",\n",
        "            },\n",
        "        {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"[Error]:{prompt}\\n\\n [code]:{code}.Please fix the error.Don't comment out anything.Generate the complete code\"\n",
        "            },\n",
        "                     {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"pipeline:{Pipeline}\\n\\n [Context]:{context}\\n\\n [Idea]:{idea} [libraries]:{libraries}\\n\\n\"\n",
        "            }\n",
        "                     ]\n",
        "\n",
        ")\n",
        "\n",
        "    # Extract and return the debugging suggestions\n",
        "    suggestions = completion.choices[0].message.content\n",
        "    print(\"Debugging suggestions:\")\n",
        "    print(suggestions)\n",
        "    python_code_match = re.search(r'```python(.+?)```', suggestions, re.DOTALL)\n",
        "    python_code = python_code_match.group(1) if python_code_match else \"\"\n",
        "\n",
        "    # Check if the error is fixed based on the suggestions\n",
        "    #new_error_message = suggestions.split(\"Error message:\")[1].strip()\n",
        "    return python_code\n",
        "\n",
        "\n",
        "\n",
        "with open('extracted_code_cleaned.py', 'r') as f:\n",
        "  python_code = f.read()\n",
        "# Extract Python code from the provided message (example)\n",
        "  python_code_blocks = re.findall(r'```python(.+?)```', python_code)\n",
        "  print(python_code_blocks)\n",
        "    # Prepare the code for execution\n",
        "  code = \"\"\"{}\n",
        "         \"\"\".format(python_code_blocks)\n",
        "\n",
        "  error_message = \"\"\n",
        "  while True:\n",
        "      try:\n",
        "        # Execute the code\n",
        "        print(code)\n",
        "        exec(code)\n",
        "        # If execution is successful, break the loop\n",
        "        print(\"Code executed successfully. No errors.\")\n",
        "        base_folder = expDetails['FolderName'].lstrip('/').replace('/', '\\\\')\n",
        "        expname = expDetails['ExpName'] + RunCount\n",
        "        filepath = os.path.join(base_folder, expname, 'Basecode.py')\n",
        "        # Ensure the directory exists\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "        with open(filepath, 'w') as f:  # Changed 'r' to 'w' for writing\n",
        "          f.write(code)\n",
        "        print(f\"Successfully wrote to {filepath}\")\n",
        "        expDetails['basecode_path'] = filepath\n",
        "\n",
        "# Write the updated JSON back to expdetails.json\n",
        "        with open('D:\\LLM_GPT\\APPCAIR\\expDetails.json', 'w') as json_file:\n",
        "          json.dump(expDetails, json_file, indent=2)\n",
        "        break\n",
        "\n",
        "      except (ValueError, Exception) as e:\n",
        "            # Extract the error message\n",
        "        error_message = str(e) + \"\\n\" + traceback.format_exc()\n",
        "        print(\"Error message:\", error_message)\n",
        "\n",
        "            # Debug the error message\n",
        "        corrected_code = debug_error(error_message, code)\n",
        "        print(error_message)\n",
        "\n",
        "        if corrected_code:\n",
        "          code = corrected_code\n",
        "        else:\n",
        "          print(\"Error correction failed. Exiting.\")\n",
        "          break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1Mwk0AxIpC3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
