<system>
you are an expert researcher, you task is to come up with an effective methodology, given an high level idea , a dataset and a model, and the literature review you did before, Use chain of thought approach to break the problem and Come up with an effective methodology to apply your literature review on the high level idea.
Keep in mind what to avoid in writing the methodology section of your research. Avoid including irrelevant details.Keep your methodology section straightforward and thorough. Details that do not contribute to the reader's understanding of your chosen methods should not be included in your methodology section. Irrelevant information includes unnecessary explanations of basic procedures. Basic procedures should only be explained if they are unconventional and unfamiliar to the readers. Do not ignore the problems you might encounter during the data gathering process. Instead of turning a blind eye, describe how you handled them 
</system>

<user>
<high_level_idea>
1. I want to use TabPFN transformer architecture for NumerAI dataset . 
2. Keep in mind the limitations of the TabPFN as given in the Model Description
3. check whether the paper you analysed is relevant to the problem.
4. if the paper is relevant combine these ideas together overcoming this limitation.
5. Else think of a different startegy to approach the problem 
6. Assume that that the entire dataset has to be trained on, and is complete, think of ways to accomplish this task with your methodology
</high_level_idea>

<model_description>
TabPFN is a small tabular classification model from the paper 'TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second'

We present TabPFN, a trained Transformer that can do supervised classification
for small tabular datasets in less than a second, needs no hyperparameter tuning
and is competitive with state-of-the-art classification methods. TabPFN performs
in-context learning (ICL), it learns to make predictions using sequences of labeled
examples (x, f(x)) given in the input, without requiring further parameter updates.
TabPFN is fully entailed in the weights of our network, which accepts training and
test samples as a set-valued input and yields predictions for the entire test set in a
single forward pass. TabPFN is a Prior-Data Fitted Network (PFN) and is trained
offline once, to approximate Bayesian inference on synthetic datasets drawn from
our prior. This prior incorporates ideas from causal reasoning: It entails a large
space of structural causal models with a preference for simple structures. On the
18 datasets in the OpenML-CC18 suite that contain up to 1 000 training data points,
up to 100 purely numerical features without missing values, and up to 10 classes,
we show that our method clearly outperforms boosted trees and performs on par
with complex state-of-the-art AutoML systems with up to 230× speedup. This
increases to a 5 700× speedup when using a GPU. We also validate these results
on an additional 67 small numerical datasets

This model has a limitation on data, features and classes, It can only be run on datasets with less than 10000 training datapoints, less than 100 features, and less than 10 classes
</model_description>

<dataset_description>
The dataset to be used is the Numeral dataset.The Numerai dataset is a tabular dataset that describes the global stock market over time.
At a high level, each row represents a stock at a specific point in time, where id is the stock id and the era is the date. The  features describe the attributes of the stock (eg. P/E ratio) known on the date and the target is a measure of future returns (eg. after 20 days) relative to the date.

It has the following bifurcation:

Features - There are many features in the dataset, ranging from fundamentals like P/E ratio, to technical signals like RSI, to market data like short interest, to secondary data like analyst ratings, and much more.
The features can be divided into 3 Feature sets a 8 Feature Groups. This is to 
The feature sets are "small", "medium", "large". - these subsets are based on the size of the dataset to load
The feature groups are "all","constitution",charisma",agility","wisdom","strength","serenity","dexterity","intelligence" - these subsets are based on common attributes of the features
Each feature has been meticulously designed and engineered by Numerai to be predictive of the target or additive to other features. We have taken extreme care to make sure all features are point-in-time to avoid leakage issues.
While many features can be predictive of the targets on their own, their predictive power is known to be inconsistent across over time. Therefore, we strongly advise against building models that rely too heavily on or are highly correlated to a small number of features as this will likely lead to inconsistent performance. See this  for more information.
Note: some features values can be NaN. This is because some feature data is just not available at that point in time, and instead of making up a fake value we are letting you choose how to deal with it yourself.

Targets - The target of the dataset is specifically engineered to match the strategy of the hedge fund.
It can be one do the following values: 0.00,0.25,0.5,0.75,1 - 5 classes
Given our hedge fund is market/country/sector and factor neutral, you can basically interpret the target as stock-specific returns that are not explained by broader trends in the market/country/sector or well known factors. In simple terms: what we are after is "alpha".
Apart from the main target we provide many auxiliary targets that are different types of stock specific returns. Like the main target, these auxiliary targets are also based on stock specific returns but are different in what is residualized (eg. market/country vs sector/factor) and time horizon (eg. 20 day vs 60 days).
Note: some auxiliary target values can be NaN but the main target will never be NaN. This is because some target data is just not available at that point in time, and instead of making up a fake value we are letting you choose how to deal with it yourself.

Eras - Eras represents different points in time, where feature values are as-of that point in time, and target values as forward looking relative to the point in time.
Instead of treating each row as a single data point, you should strongly consider treating each era as a single data point. For this same reason, many of the metrics on Numerai are "per-era", for example mean correlation per-era.
In historical data (train, validation), eras are 1 week apart but the target values can be forward looking by 20 days or 60 days. This means that the target values are "overlapping" so special care must be taken when applying cross validation. See this  for more information.
</dataset_description>

<reasoning>
**Instructions for writing an effective methodology section**
1. **Introduce your methods**: 
    Introduce the methodological approach used in investigating your research problem. In one of the previous sections, your methodological approach can either be quantitative, qualitative, or mixed methods. Look for a methodology in research example that you can use as a reference.
2. **Establish methodological connection**: 
    Explain the relevance of your methodological approach to the overall research design. Keep in mind that the connection between your methods and your research problem should be clear. This means that your methodology of research must be appropriate to achieve your paper’s objective—to address the research problem you presented. To wit, if you need help to write your research problem, refer to our article on what is a research question.
3. **Introduce your instruments**: 
    Indicate the research instruments you are going to use in collecting your data and explain how you are going to use them. These tools and instruments can be your surveys, questionnaires for interviews, observation, etc. If your methods include archival research or analyzing existing data, provide background information for documents, including who the original researcher is, as well as how the data were originally created and gathered. Keep in mind that aside from your methodology in research paper, the identification of the research instrument is equally significant.
4. **Discuss your analysis**: 
    Explain how you are going to analyze the results of your data gathering process. Depending on your methodology, research for ways on how you can best execute your study either by using statistical analysis or exploring theoretical perspectives to support your explanation of observed behaviors.
5. **Provide background information**: 
    When using methods that your readers may be unfamiliar with, make sure to provide background information about these methods. It would also help if you can provide your research methodology meaning so you can present a clear and comprehensive research context.
6. **Discuss sampling process**: 
    Sampling procedures are vital components of your methodology. Explain the reason behind your sampling procedure. For example, if you are using statistics in your research, indicate why you chose this method as well as your sampling procedure. If you are going to do interviews, describe how are you going to choose the participants and how the interviews will be conducted.
7. **Address research limitations**: 
    Make sure to address possible limitations you may encounter in your research, such as practical limitations that may affect your data gathering process. If there are potential issues you anticipate to encounter in the process, indicate your reason why you still decide to use the methodology despite the risk
</reasoning>

<methodology> 
your methodology as a step by step method which can be given for coding after answering the questions
</methodology> 

<pseudocode_generation>
your pseudocode for the methodology as a step by step approach
</pseudocode_generation>
</user>

